{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA1361 - Model-Based Decision Making\n",
    "# Week 3 - Sensitivity analysis\n",
    "\n",
    "This exercise uses the same predator-prey model we used for the multi-model exercise, focusing on the Python version. As with the other exercise, define a model object for the function below, with the uncertainty ranges provided:\n",
    "\n",
    "|Parameter\t|Range or value\t        |\n",
    "|-----------|--------------:|\n",
    "|prey_birth_rate    \t|0.015 – 0.035\t|\n",
    "|predation_rate|0.0005 – 0.003 \t|\n",
    "|predator_efficiency     \t|0.001 – 0.004\t    |\n",
    "|predator_loss_rate\t    |0.04 – 0.08\t    |\n",
    "\n",
    "* Sensitivity analysis often focuses on the final values of an outcome at the end of the simulation. However, we can also look at metrics that give us additional information about the behavior of the model over time. Using [the statsmodel library](https://www.statsmodels.org/stable/index.html) and an appropriate sampling design, fit a linear regression model for each of the following indicators. What can we conclude about the behavior of the model, and about the importance of the different inputs?\n",
    "\n",
    "  * The final values of the _prey_ outcome\n",
    "  * The mean values of the _prey_ outcome over time, within each experiment\n",
    "  * The standard deviations of the _prey_ outcome over time, within each experiment\n",
    "  \n",
    "\n",
    "* Use the Sobol sampling functionality included in the Workbench to perform experiments with a sample size of N=50, then analyze the results with SALib for the same three indicators. This requires specifying the keyword argument `'uncertainty_sampling'` of perform_experiments. Note that when using Sobol sampling, the meaning of the keyword argument `scenarios` changes a bit. In order to properly estimate Sobol scores as well as interaction effects, you require N * (2D+2) scenarios, where D is the number of uncertain parameters, and N is the value for scenarios passed to `perform_experiments`. Repeat the analysis for larger sample sizes, with N=250 and N=1000. How can we interpret the first-order and total indices? Are these sample sizes sufficient for a stable estimation of the indices? You'll need to use the [get_SALib_problem](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/salib_samplers.html) function to convert your Workbench experiments to a problem definition that you can pass to the SALib analysis function. \n",
    "\n",
    "* *hint*: sobol is a deterministic sequence of quasi random numbers. Thus, you can run with N=1000 and simply slice for 1:50 and 1:250.\n",
    "\n",
    "* Use the [Extra-Trees analysis](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/feature_scoring.html) included in the Workbench to approximate the Sobol total indices, with a suitable sampling design. As a starting point, use an ensemble of 100 trees and a max_features parameter of 0.6, and set the analysis to regression mode. Are the estimated importances stable relative to the sample size and the analysis parameters? How do the results compare to the Sobol indices? For more details on this analysis see [Jaxa-Rozen & Kwakkel (2018)](https://www.sciencedirect.com/science/article/pii/S1364815217311581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "from ema_workbench import (Model, RealParameter, TimeSeriesOutcome, perform_experiments, ema_logging, MultiprocessingEvaluator)\n",
    "\n",
    "from ema_workbench.em_framework.evaluators import LHS, SOBOL, MORRIS\n",
    "\n",
    "from ema_workbench.analysis import feature_scoring\n",
    "from ema_workbench.analysis.scenario_discovery_util import RuleInductionType\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "from SALib.analyze import sobol\n",
    "from statistics import mean\n",
    "\n",
    "def pred_prey(prey_birth_rate=0.025, predation_rate=0.0015, predator_efficiency=0.002,\n",
    "             predator_loss_rate=0.06, initial_prey=50, initial_predators=20, dt=0.25, \n",
    "             final_time=365, reps=1):\n",
    "\n",
    "    #Initial values\n",
    "    predators = np.zeros((reps, int(final_time/dt)+1))\n",
    "    prey = np.zeros((reps, int(final_time/dt)+1))\n",
    "    sim_time = np.zeros((reps, int(final_time/dt)+1))\n",
    "    \n",
    "    for r in range(reps):\n",
    "\n",
    "        predators[r,0] = initial_predators\n",
    "        prey[r,0] = initial_prey\n",
    "\n",
    "    #Calculate the time series\n",
    "    for t in range(0, sim_time.shape[1]-1):\n",
    "\n",
    "        dx = (prey_birth_rate*prey[r,t]) - (predation_rate*prey[r,t]*predators[r,t])\n",
    "        dy = (predator_efficiency*predators[r,t]*prey[r,t]) - (predator_loss_rate*predators[r,t])\n",
    "\n",
    "        prey[r,t+1] = max(prey[r,t] + dx*dt, 0)\n",
    "        predators[r,t+1] = max(predators[r,t] + dy*dt, 0)\n",
    "        sim_time[r,t+1] = (t+1)*dt\n",
    "    \n",
    "    #Return outcomes\n",
    "    return {'TIME':sim_time,\n",
    "            'predators':predators,\n",
    "            'prey':prey}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "#We can define common uncertainties and outcomes for each model:\n",
    "uncertainties = [RealParameter('prey_birth_rate', 0.015, 0.035),\n",
    "                 RealParameter('predation_rate', 0.0005, 0.003),\n",
    "                 RealParameter('predator_efficiency', 0.001, 0.004),\n",
    "                 RealParameter('predator_loss_rate', 0.04, 0.08)] \n",
    "\n",
    "outcomes = [TimeSeriesOutcome('TIME', function=np.squeeze),\n",
    "            TimeSeriesOutcome('predators', function=np.squeeze),\n",
    "            TimeSeriesOutcome('prey', function=np.squeeze)]\n",
    "\n",
    "\n",
    "#Define the Python model\n",
    "py_model = Model('Python', function=pred_prey)\n",
    "py_model.uncertainties = uncertainties\n",
    "py_model.outcomes = outcomes\n",
    "\n",
    "\n",
    "nr_experiments = 100\n",
    "\n",
    "#Using Latin Hypercube sampling\n",
    "experiments, outcomes = perform_experiments(py_model, nr_experiments, uncertainty_sampling=LHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate final outcomes\n",
    "final_outcomes_prey = []\n",
    "\n",
    "for i in range(len(outcomes['prey'])):\n",
    "    final_outcomes_prey.append((outcomes['prey'][i][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean values of prey over time \n",
    "mean_outcomes_prey = []\n",
    "\n",
    "for i in range(len(outcomes['prey'])):\n",
    "    mean_outcomes_prey.append(mean((outcomes['prey'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard deviation over time\n",
    "std_outcomes_prey = []\n",
    "\n",
    "for i in range(len(outcomes['prey'])):\n",
    "    std_outcomes_prey.append(np.std(((outcomes['prey'][i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model for the final outcomes\n",
    "x = experiments[['predation_rate', 'predator_efficiency', 'predator_loss_rate', 'prey_birth_rate']]\n",
    "\n",
    "reg_mod_final = sm.OLS(final_outcomes_prey, x)\n",
    "res_fin = reg_mod_final.fit()\n",
    "\n",
    "\n",
    "print(res_fin.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(res_fin, 0, ax=ax)\n",
    "fig.set_size_inches(10, 6, forward=True)\n",
    "ax.set_ylabel(\"Final outcomes prey\")\n",
    "ax.set_xlabel(\"Uncertainties\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model for the mean values over time\n",
    "\n",
    "x = experiments[['predation_rate', 'predator_efficiency', 'predator_loss_rate', 'prey_birth_rate']]\n",
    "\n",
    "reg_mod_mean = sm.OLS(mean_outcomes_prey, x)\n",
    "res_mean = reg_mod_mean.fit()\n",
    "\n",
    "\n",
    "print(res_mean.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(res_mean, 0, ax=ax)\n",
    "fig.set_size_inches(12, 8, forward=True)\n",
    "ax.set_ylabel(\"Mean values prey\")\n",
    "ax.set_xlabel(\"Uncertainties\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model for the standard deviations over time\n",
    "\n",
    "x = experiments[['predation_rate', 'predator_efficiency', 'predator_loss_rate', 'prey_birth_rate']]\n",
    "\n",
    "reg_mod_std = sm.OLS(std_outcomes_prey, x)\n",
    "res_std = reg_mod_std.fit()\n",
    "\n",
    "print(res_std.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(res_std, 0, ax=ax)\n",
    "fig.set_size_inches(12, 8, forward=True)\n",
    "ax.set_ylabel(\"Mean values prey\")\n",
    "ax.set_xlabel(\"Uncertainties\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOBOL sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.analyze import sobol\n",
    "from ema_workbench.em_framework.salib_samplers import get_SALib_problem\n",
    "\n",
    "sa_results = perform_experiments(py_model, scenarios=50, uncertainty_sampling='sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = sa_results\n",
    "\n",
    "problem = get_SALib_problem(py_model.uncertainties)\n",
    "Si = sobol.analyze(problem, outcomes['prey'][:,-1],\n",
    "                   calc_second_order=True, print_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
